{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOsTUZmpBJ5U"
      },
      "source": [
        "# Import modules and packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from datetime import timedelta\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas.plotting import register_matplotlib_converters\n",
        "register_matplotlib_converters()\n",
        "from time import time\n",
        "import math\n",
        "import seaborn as sns\n",
        "import datetime as dt\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from sklearn.utils import resample\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QudEabeBLP2"
      },
      "source": [
        "#Importing Training Set\n",
        "df = pd.read_csv('Dataset.csv')\n",
        "\n",
        "dataset_train = pd.read_csv('Dataset.csv')\n",
        "#Separate dates for future plotting"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yg1o4IthBLS4",
        "outputId": "931bc95d-ac8b-40a5-e713-917bf26a01fc"
      },
      "source": [
        "# Select features (columns) to be involved intro training and predictions\n",
        "cols = list(dataset_train)[1:9]\n",
        "\n",
        "# Extract dates (will be used in visualization)\n",
        "datelist_train = list(dataset_train['DATETIME'])\n",
        "datelist_train = [dt.datetime.strptime(date, '%d/%m/%y %H').date() for date in datelist_train]\n",
        "#print(datelist_train)\n",
        "#print(dataset_train)\n",
        "print('Training set shape == {}'.format(dataset_train.shape))\n",
        "print('All timestamps == {}'.format(len(datelist_train)))\n",
        "print('Featured selected: {}'.format(cols))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set shape == (6266, 12)\n",
            "All timestamps == 6266\n",
            "Featured selected: [' L_T1', ' L_T2', ' L_T3', ' L_T4', ' L_T5', ' L_T6', ' L_T7', 'FLAG']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNEDXO7PBLYS",
        "outputId": "6d505cdc-bf8a-4579-8f7a-813f87c25668"
      },
      "source": [
        "dataset = df[cols].astype(str)\n",
        "#print(len(dataset1))\n",
        "#for i in cols:\n",
        "  #for j in range(0, len(dataset1)):\n",
        "      #dataset[i][j] = dataset1[i][j].replace(',', '')\n",
        "\n",
        "\n",
        "dataset_train = dataset[:].astype(float)\n",
        "dataset_test = dataset[4177:].astype(float)\n",
        "\n",
        "dataset_T = dataset.astype(float).values   \n",
        "# Using multiple features (predictors)\n",
        "training_set = dataset_train.values\n",
        "testing_set = dataset_test.values\n",
        "\n",
        "print('Shape of training set == {}.'.format(training_set.shape))\n",
        "print('Shape of testing set == {}.'.format(testing_set.shape))\n",
        "\n",
        "print(testing_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of training set == (6266, 8).\n",
            "Shape of testing set == (2089, 8).\n",
            "[[0.73 2.27 4.   ... 5.5  4.28 0.  ]\n",
            " [0.69 2.25 4.53 ... 5.5  4.78 0.  ]\n",
            " [0.9  2.31 5.03 ... 5.16 3.22 0.  ]\n",
            " ...\n",
            " [1.07 2.24 2.99 ... 5.12 1.78 0.  ]\n",
            " [0.85 2.14 3.44 ... 5.24 1.82 0.  ]\n",
            " [0.74 2.11 3.89 ... 5.29 1.62 0.  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lr5YD9T0BLbK",
        "outputId": "2e204a83-23c9-4229-80da-adbd85af0c78"
      },
      "source": [
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
        "#sc = MinMaxScaler(feature_range=(0, 1))\n",
        "sc = StandardScaler()\n",
        "datasetTotal=dataset_train.values\n",
        "dataset_scaled = sc.fit_transform(datasetTotal[:, 0:7])\n",
        "\n",
        "training_set_scaled= dataset_scaled[:4177]\n",
        "test_set_scaled =dataset_scaled[4177:]\n",
        "print(training_set_scaled.shape)\n",
        "print(test_set_scaled.shape)\n",
        "\n",
        "labelX = datasetTotal[:4177, 7:8]\n",
        "labelY = datasetTotal[4177:, 7:8]\n",
        "print(labelY.shape)\n",
        "print(labelX.shape)\n",
        "\n",
        "#sc_predict = MinMaxScaler(feature_range=(0, 1))\n",
        "#sc_predict.fit_transform(dataset_T[:, 0:])\n",
        "#labelX = np.reshape(labelX, (labelX.shape[0], 1, labelX.shape[1]))\n",
        "\n",
        "#trainX = np.reshape(training_set_scaled, (training_set_scaled.shape[0], 1, training_set_scaled.shape[1]))\n",
        "\n",
        "#print(training_set_scaled)\n",
        "print(labelX.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4177, 7)\n",
            "(2089, 7)\n",
            "(2089, 1)\n",
            "(4177, 1)\n",
            "(4177, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFHPpgnnFoDV",
        "outputId": "25daeb0d-6c93-4bc2-dd63-ced233cc6ba9"
      },
      "source": [
        "\n",
        "labelX = np.reshape(labelX, (labelX.shape[0],))\n",
        "...\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(labelX)\n",
        "encoded_Y = encoder.transform(labelX)\n",
        "print(labelX.shape)\n",
        "weight_for_0 = 1.0 / 3958\n",
        "weight_for_1 = 1.0 / 219\n",
        "print(labelX)\n",
        "print(weight_for_0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4177,)\n",
            "[0. 0. 0. ... 0. 0. 0.]\n",
            "0.00025265285497726126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meQ6DV3j5DPS",
        "outputId": "08f4a7b8-e6d9-4d4b-8280-ccf4cce72224"
      },
      "source": [
        "\n",
        "labelY = np.reshape(labelY, (labelY.shape[0],))\n",
        "...\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(labelY)\n",
        "encoded_TY = encoder.transform(labelY)\n",
        "print(encoded_TY.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2089,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL_il8H1BLx5"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import LSTM, GRU, Conv1D, Conv1DTranspose\n",
        "from keras.layers import Dropout, Activation\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "#from tensorflow.keras.metrics.Precision\n",
        "from keras import regularizers\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.metrics import AUC\n",
        "from sklearn.metrics import roc_auc_score, auc\n",
        "from keras import metrics\n",
        "EPOCHS = 200\n",
        "INIT_LR = 1e-3\n",
        "BS = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYQgyiaMBL0Z"
      },
      "source": [
        "  ## Initializing the Neural Network based on LSTM\n",
        "#def create_baseline():\n",
        "model = Sequential()\n",
        "model.add(Dense(units=512, input_dim=7, activation= 'relu', kernel_regularizer=regularizers.l1_l2(l1=0.00001, l2=0.0007)))\n",
        "    #model.add(BatchNormalization())\n",
        "    #model.add(Activation('relu'))\n",
        "            \n",
        "\n",
        "    #model.add(Dropout(0.1))\n",
        "model.add(Dense(units=256, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.00001, l2=0.0007)))\n",
        "#model.add(Dropout(0.1))\n",
        "model.add(Dense(units=128, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.00001, l2=0.0007)))\n",
        "#model.add(Dropout(0.1))\n",
        "    #model.add(Dense(units=128, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.00001, l2=0.0007)))\n",
        "    #model.add(Dropout(0.1))\n",
        "    #model.add(Dense(units=64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.00001, l2=0.0007)))\n",
        "    #model.add(Dropout(0.1))\n",
        "model.add(Dense(units=64, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.00001, l2=0.0007)))\n",
        "#model.add(Dropout(0.1))\n",
        "model.add(Dense(units=32, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.00001, l2=0.0007)))\n",
        "#model.add(Dropout(0.1))\n",
        "model.add(Dense(units=16, activation='relu', kernel_regularizer=regularizers.l1_l2(l1=0.00001, l2=0.0007)))\n",
        "#model.add(Dropout(0.1))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "    #opt = Adam(learning_rate=0.001, decay=0.000003)\n",
        "opt= Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "\n",
        "metrics = [\n",
        "    metrics.FalseNegatives(name=\"fn\"),\n",
        "    metrics.FalsePositives(name=\"fp\"),\n",
        "    metrics.TrueNegatives(name=\"tn\"),\n",
        "    metrics.TruePositives(name=\"tp\"),\n",
        "    metrics.Precision(name=\"precision\"),\n",
        "    metrics.Recall(name=\"recall\"),\n",
        "]\n",
        "model.compile(loss='binary_crossentropy', optimizer= opt, metrics=['accuracy'])\n",
        "    #return model    \n",
        "\n",
        "\n",
        "#Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
        "#model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNpFd1yzdksg"
      },
      "source": [
        "def define_model():\n",
        "    model = Sequential()\n",
        "    #define first hidden layer and visible layer , kernel_initializer='he_uniform'\n",
        "    model.add(Dense(512, input_dim=7, activation='relu', kernel_regularizer=regularizers.l2(l2=0.005)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(l2=0.005)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(l2=0.005)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l2(l2=0.005)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(l2=0.005)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(l2=0.005)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    opt= Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "\n",
        "    lr_schedule = ExponentialDecay(\n",
        "    initial_learning_rate=1e-2,\n",
        "    decay_steps=50,\n",
        "    decay_rate=0.9)\n",
        "    optimizer = SGD(learning_rate=lr_schedule)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR0AfpmDI-Hg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b10d0bd-74ad-4753-82ae-497fc652c483"
      },
      "source": [
        "model = define_model()\n",
        "class_weight = {0: 1, 1: 1}\n",
        "history= model.fit(training_set_scaled, encoded_Y, shuffle=False, epochs=500, batch_size=32, validation_split=0.3, class_weight=class_weight, verbose=1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "92/92 [==============================] - 2s 9ms/step - loss: 2.2325 - accuracy: 0.9436 - val_loss: 0.9007 - val_accuracy: 0.9362\n",
            "Epoch 2/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.6333 - accuracy: 0.9524 - val_loss: 0.5260 - val_accuracy: 0.9362\n",
            "Epoch 3/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.4343 - accuracy: 0.9524 - val_loss: 0.4318 - val_accuracy: 0.9362\n",
            "Epoch 4/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.3618 - accuracy: 0.9524 - val_loss: 0.3862 - val_accuracy: 0.9362\n",
            "Epoch 5/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.3276 - accuracy: 0.9524 - val_loss: 0.3576 - val_accuracy: 0.9362\n",
            "Epoch 6/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.3077 - accuracy: 0.9524 - val_loss: 0.3369 - val_accuracy: 0.9362\n",
            "Epoch 7/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2928 - accuracy: 0.9524 - val_loss: 0.3214 - val_accuracy: 0.9362\n",
            "Epoch 8/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2812 - accuracy: 0.9524 - val_loss: 0.3102 - val_accuracy: 0.9362\n",
            "Epoch 9/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2721 - accuracy: 0.9524 - val_loss: 0.3026 - val_accuracy: 0.9362\n",
            "Epoch 10/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2693 - accuracy: 0.9524 - val_loss: 0.2951 - val_accuracy: 0.9362\n",
            "Epoch 11/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2616 - accuracy: 0.9524 - val_loss: 0.2883 - val_accuracy: 0.9362\n",
            "Epoch 12/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2544 - accuracy: 0.9524 - val_loss: 0.2836 - val_accuracy: 0.9362\n",
            "Epoch 13/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2532 - accuracy: 0.9524 - val_loss: 0.2804 - val_accuracy: 0.9362\n",
            "Epoch 14/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2514 - accuracy: 0.9524 - val_loss: 0.2761 - val_accuracy: 0.9362\n",
            "Epoch 15/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2431 - accuracy: 0.9524 - val_loss: 0.2727 - val_accuracy: 0.9362\n",
            "Epoch 16/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2410 - accuracy: 0.9524 - val_loss: 0.2700 - val_accuracy: 0.9362\n",
            "Epoch 17/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2393 - accuracy: 0.9524 - val_loss: 0.2676 - val_accuracy: 0.9362\n",
            "Epoch 18/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2372 - accuracy: 0.9524 - val_loss: 0.2654 - val_accuracy: 0.9362\n",
            "Epoch 19/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2384 - accuracy: 0.9524 - val_loss: 0.2637 - val_accuracy: 0.9362\n",
            "Epoch 20/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2347 - accuracy: 0.9524 - val_loss: 0.2615 - val_accuracy: 0.9362\n",
            "Epoch 21/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2308 - accuracy: 0.9524 - val_loss: 0.2597 - val_accuracy: 0.9362\n",
            "Epoch 22/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2312 - accuracy: 0.9524 - val_loss: 0.2580 - val_accuracy: 0.9362\n",
            "Epoch 23/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2282 - accuracy: 0.9524 - val_loss: 0.2563 - val_accuracy: 0.9362\n",
            "Epoch 24/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2278 - accuracy: 0.9524 - val_loss: 0.2548 - val_accuracy: 0.9362\n",
            "Epoch 25/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2285 - accuracy: 0.9524 - val_loss: 0.2535 - val_accuracy: 0.9362\n",
            "Epoch 26/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2244 - accuracy: 0.9524 - val_loss: 0.2521 - val_accuracy: 0.9362\n",
            "Epoch 27/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2210 - accuracy: 0.9524 - val_loss: 0.2509 - val_accuracy: 0.9362\n",
            "Epoch 28/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2214 - accuracy: 0.9524 - val_loss: 0.2499 - val_accuracy: 0.9362\n",
            "Epoch 29/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2179 - accuracy: 0.9524 - val_loss: 0.2490 - val_accuracy: 0.9362\n",
            "Epoch 30/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2164 - accuracy: 0.9524 - val_loss: 0.2482 - val_accuracy: 0.9362\n",
            "Epoch 31/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2171 - accuracy: 0.9524 - val_loss: 0.2475 - val_accuracy: 0.9362\n",
            "Epoch 32/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2156 - accuracy: 0.9524 - val_loss: 0.2468 - val_accuracy: 0.9362\n",
            "Epoch 33/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2124 - accuracy: 0.9524 - val_loss: 0.2463 - val_accuracy: 0.9362\n",
            "Epoch 34/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2151 - accuracy: 0.9524 - val_loss: 0.2457 - val_accuracy: 0.9362\n",
            "Epoch 35/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2132 - accuracy: 0.9524 - val_loss: 0.2453 - val_accuracy: 0.9362\n",
            "Epoch 36/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2120 - accuracy: 0.9524 - val_loss: 0.2448 - val_accuracy: 0.9362\n",
            "Epoch 37/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2125 - accuracy: 0.9524 - val_loss: 0.2444 - val_accuracy: 0.9362\n",
            "Epoch 38/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2096 - accuracy: 0.9524 - val_loss: 0.2440 - val_accuracy: 0.9362\n",
            "Epoch 39/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2089 - accuracy: 0.9524 - val_loss: 0.2437 - val_accuracy: 0.9362\n",
            "Epoch 40/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2095 - accuracy: 0.9524 - val_loss: 0.2433 - val_accuracy: 0.9362\n",
            "Epoch 41/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2101 - accuracy: 0.9524 - val_loss: 0.2430 - val_accuracy: 0.9362\n",
            "Epoch 42/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2084 - accuracy: 0.9524 - val_loss: 0.2428 - val_accuracy: 0.9362\n",
            "Epoch 43/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2041 - accuracy: 0.9524 - val_loss: 0.2426 - val_accuracy: 0.9362\n",
            "Epoch 44/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2105 - accuracy: 0.9524 - val_loss: 0.2422 - val_accuracy: 0.9362\n",
            "Epoch 45/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2070 - accuracy: 0.9524 - val_loss: 0.2420 - val_accuracy: 0.9362\n",
            "Epoch 46/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2057 - accuracy: 0.9524 - val_loss: 0.2418 - val_accuracy: 0.9362\n",
            "Epoch 47/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2081 - accuracy: 0.9524 - val_loss: 0.2415 - val_accuracy: 0.9362\n",
            "Epoch 48/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2082 - accuracy: 0.9524 - val_loss: 0.2413 - val_accuracy: 0.9362\n",
            "Epoch 49/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2055 - accuracy: 0.9524 - val_loss: 0.2412 - val_accuracy: 0.9362\n",
            "Epoch 50/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2075 - accuracy: 0.9524 - val_loss: 0.2410 - val_accuracy: 0.9362\n",
            "Epoch 51/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2045 - accuracy: 0.9524 - val_loss: 0.2408 - val_accuracy: 0.9362\n",
            "Epoch 52/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2042 - accuracy: 0.9524 - val_loss: 0.2407 - val_accuracy: 0.9362\n",
            "Epoch 53/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2031 - accuracy: 0.9524 - val_loss: 0.2406 - val_accuracy: 0.9362\n",
            "Epoch 54/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2065 - accuracy: 0.9524 - val_loss: 0.2404 - val_accuracy: 0.9362\n",
            "Epoch 55/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2056 - accuracy: 0.9524 - val_loss: 0.2403 - val_accuracy: 0.9362\n",
            "Epoch 56/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2087 - accuracy: 0.9524 - val_loss: 0.2400 - val_accuracy: 0.9362\n",
            "Epoch 57/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2021 - accuracy: 0.9524 - val_loss: 0.2401 - val_accuracy: 0.9362\n",
            "Epoch 58/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2032 - accuracy: 0.9524 - val_loss: 0.2400 - val_accuracy: 0.9362\n",
            "Epoch 59/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2018 - accuracy: 0.9524 - val_loss: 0.2400 - val_accuracy: 0.9362\n",
            "Epoch 60/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2021 - accuracy: 0.9524 - val_loss: 0.2398 - val_accuracy: 0.9362\n",
            "Epoch 61/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2013 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 62/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2052 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 63/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2067 - accuracy: 0.9524 - val_loss: 0.2394 - val_accuracy: 0.9362\n",
            "Epoch 64/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2019 - accuracy: 0.9524 - val_loss: 0.2394 - val_accuracy: 0.9362\n",
            "Epoch 65/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2028 - accuracy: 0.9524 - val_loss: 0.2394 - val_accuracy: 0.9362\n",
            "Epoch 66/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2019 - accuracy: 0.9524 - val_loss: 0.2394 - val_accuracy: 0.9362\n",
            "Epoch 67/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2002 - accuracy: 0.9524 - val_loss: 0.2393 - val_accuracy: 0.9362\n",
            "Epoch 68/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2025 - accuracy: 0.9524 - val_loss: 0.2392 - val_accuracy: 0.9362\n",
            "Epoch 69/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2009 - accuracy: 0.9524 - val_loss: 0.2392 - val_accuracy: 0.9362\n",
            "Epoch 70/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2019 - accuracy: 0.9524 - val_loss: 0.2391 - val_accuracy: 0.9362\n",
            "Epoch 71/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2018 - accuracy: 0.9524 - val_loss: 0.2390 - val_accuracy: 0.9362\n",
            "Epoch 72/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1996 - accuracy: 0.9524 - val_loss: 0.2391 - val_accuracy: 0.9362\n",
            "Epoch 73/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2017 - accuracy: 0.9524 - val_loss: 0.2390 - val_accuracy: 0.9362\n",
            "Epoch 74/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2035 - accuracy: 0.9524 - val_loss: 0.2390 - val_accuracy: 0.9362\n",
            "Epoch 75/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2011 - accuracy: 0.9524 - val_loss: 0.2391 - val_accuracy: 0.9362\n",
            "Epoch 76/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2035 - accuracy: 0.9524 - val_loss: 0.2388 - val_accuracy: 0.9362\n",
            "Epoch 77/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2008 - accuracy: 0.9524 - val_loss: 0.2388 - val_accuracy: 0.9362\n",
            "Epoch 78/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2023 - accuracy: 0.9524 - val_loss: 0.2387 - val_accuracy: 0.9362\n",
            "Epoch 79/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1997 - accuracy: 0.9524 - val_loss: 0.2388 - val_accuracy: 0.9362\n",
            "Epoch 80/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1989 - accuracy: 0.9524 - val_loss: 0.2388 - val_accuracy: 0.9362\n",
            "Epoch 81/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.1997 - accuracy: 0.9524 - val_loss: 0.2389 - val_accuracy: 0.9362\n",
            "Epoch 82/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2017 - accuracy: 0.9524 - val_loss: 0.2387 - val_accuracy: 0.9362\n",
            "Epoch 83/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2002 - accuracy: 0.9524 - val_loss: 0.2386 - val_accuracy: 0.9362\n",
            "Epoch 84/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2001 - accuracy: 0.9524 - val_loss: 0.2387 - val_accuracy: 0.9362\n",
            "Epoch 85/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2025 - accuracy: 0.9524 - val_loss: 0.2385 - val_accuracy: 0.9362\n",
            "Epoch 86/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2037 - accuracy: 0.9524 - val_loss: 0.2385 - val_accuracy: 0.9362\n",
            "Epoch 87/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2001 - accuracy: 0.9524 - val_loss: 0.2385 - val_accuracy: 0.9362\n",
            "Epoch 88/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2012 - accuracy: 0.9524 - val_loss: 0.2385 - val_accuracy: 0.9362\n",
            "Epoch 89/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2009 - accuracy: 0.9524 - val_loss: 0.2385 - val_accuracy: 0.9362\n",
            "Epoch 90/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.1994 - accuracy: 0.9524 - val_loss: 0.2385 - val_accuracy: 0.9362\n",
            "Epoch 91/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1981 - accuracy: 0.9524 - val_loss: 0.2386 - val_accuracy: 0.9362\n",
            "Epoch 92/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1990 - accuracy: 0.9524 - val_loss: 0.2385 - val_accuracy: 0.9362\n",
            "Epoch 93/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1996 - accuracy: 0.9524 - val_loss: 0.2386 - val_accuracy: 0.9362\n",
            "Epoch 94/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2010 - accuracy: 0.9524 - val_loss: 0.2384 - val_accuracy: 0.9362\n",
            "Epoch 95/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2012 - accuracy: 0.9524 - val_loss: 0.2384 - val_accuracy: 0.9362\n",
            "Epoch 96/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2030 - accuracy: 0.9524 - val_loss: 0.2382 - val_accuracy: 0.9362\n",
            "Epoch 97/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1974 - accuracy: 0.9524 - val_loss: 0.2384 - val_accuracy: 0.9362\n",
            "Epoch 98/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2002 - accuracy: 0.9524 - val_loss: 0.2383 - val_accuracy: 0.9362\n",
            "Epoch 99/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1993 - accuracy: 0.9524 - val_loss: 0.2384 - val_accuracy: 0.9362\n",
            "Epoch 100/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1987 - accuracy: 0.9524 - val_loss: 0.2385 - val_accuracy: 0.9362\n",
            "Epoch 101/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2003 - accuracy: 0.9524 - val_loss: 0.2384 - val_accuracy: 0.9362\n",
            "Epoch 102/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1993 - accuracy: 0.9524 - val_loss: 0.2384 - val_accuracy: 0.9362\n",
            "Epoch 103/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1973 - accuracy: 0.9524 - val_loss: 0.2384 - val_accuracy: 0.9362\n",
            "Epoch 104/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2005 - accuracy: 0.9524 - val_loss: 0.2384 - val_accuracy: 0.9362\n",
            "Epoch 105/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1989 - accuracy: 0.9524 - val_loss: 0.2384 - val_accuracy: 0.9362\n",
            "Epoch 106/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1982 - accuracy: 0.9524 - val_loss: 0.2385 - val_accuracy: 0.9362\n",
            "Epoch 107/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1999 - accuracy: 0.9524 - val_loss: 0.2383 - val_accuracy: 0.9362\n",
            "Epoch 108/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1984 - accuracy: 0.9524 - val_loss: 0.2384 - val_accuracy: 0.9362\n",
            "Epoch 109/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1969 - accuracy: 0.9524 - val_loss: 0.2386 - val_accuracy: 0.9362\n",
            "Epoch 110/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1971 - accuracy: 0.9524 - val_loss: 0.2386 - val_accuracy: 0.9362\n",
            "Epoch 111/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1987 - accuracy: 0.9524 - val_loss: 0.2384 - val_accuracy: 0.9362\n",
            "Epoch 112/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1985 - accuracy: 0.9524 - val_loss: 0.2384 - val_accuracy: 0.9362\n",
            "Epoch 113/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.1981 - accuracy: 0.9524 - val_loss: 0.2384 - val_accuracy: 0.9362\n",
            "Epoch 114/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2008 - accuracy: 0.9524 - val_loss: 0.2383 - val_accuracy: 0.9362\n",
            "Epoch 115/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1987 - accuracy: 0.9524 - val_loss: 0.2383 - val_accuracy: 0.9362\n",
            "Epoch 116/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2006 - accuracy: 0.9524 - val_loss: 0.2382 - val_accuracy: 0.9362\n",
            "Epoch 117/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.1993 - accuracy: 0.9524 - val_loss: 0.2382 - val_accuracy: 0.9362\n",
            "Epoch 118/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1994 - accuracy: 0.9524 - val_loss: 0.2384 - val_accuracy: 0.9362\n",
            "Epoch 119/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1969 - accuracy: 0.9524 - val_loss: 0.2384 - val_accuracy: 0.9362\n",
            "Epoch 120/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1982 - accuracy: 0.9524 - val_loss: 0.2384 - val_accuracy: 0.9362\n",
            "Epoch 121/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1991 - accuracy: 0.9524 - val_loss: 0.2385 - val_accuracy: 0.9362\n",
            "Epoch 122/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1992 - accuracy: 0.9524 - val_loss: 0.2383 - val_accuracy: 0.9362\n",
            "Epoch 123/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1962 - accuracy: 0.9524 - val_loss: 0.2385 - val_accuracy: 0.9362\n",
            "Epoch 124/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1982 - accuracy: 0.9524 - val_loss: 0.2384 - val_accuracy: 0.9362\n",
            "Epoch 125/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1991 - accuracy: 0.9524 - val_loss: 0.2384 - val_accuracy: 0.9362\n",
            "Epoch 126/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.2013 - accuracy: 0.9524 - val_loss: 0.2384 - val_accuracy: 0.9362\n",
            "Epoch 127/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1957 - accuracy: 0.9524 - val_loss: 0.2384 - val_accuracy: 0.9362\n",
            "Epoch 128/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1960 - accuracy: 0.9524 - val_loss: 0.2385 - val_accuracy: 0.9362\n",
            "Epoch 129/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1959 - accuracy: 0.9524 - val_loss: 0.2386 - val_accuracy: 0.9362\n",
            "Epoch 130/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1975 - accuracy: 0.9524 - val_loss: 0.2386 - val_accuracy: 0.9362\n",
            "Epoch 131/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1991 - accuracy: 0.9524 - val_loss: 0.2385 - val_accuracy: 0.9362\n",
            "Epoch 132/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1990 - accuracy: 0.9524 - val_loss: 0.2385 - val_accuracy: 0.9362\n",
            "Epoch 133/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1972 - accuracy: 0.9524 - val_loss: 0.2384 - val_accuracy: 0.9362\n",
            "Epoch 134/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1957 - accuracy: 0.9524 - val_loss: 0.2386 - val_accuracy: 0.9362\n",
            "Epoch 135/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1973 - accuracy: 0.9524 - val_loss: 0.2386 - val_accuracy: 0.9362\n",
            "Epoch 136/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1960 - accuracy: 0.9524 - val_loss: 0.2387 - val_accuracy: 0.9362\n",
            "Epoch 137/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1979 - accuracy: 0.9524 - val_loss: 0.2385 - val_accuracy: 0.9362\n",
            "Epoch 138/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1967 - accuracy: 0.9524 - val_loss: 0.2385 - val_accuracy: 0.9362\n",
            "Epoch 139/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.2006 - accuracy: 0.9524 - val_loss: 0.2385 - val_accuracy: 0.9362\n",
            "Epoch 140/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1957 - accuracy: 0.9524 - val_loss: 0.2387 - val_accuracy: 0.9362\n",
            "Epoch 141/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1961 - accuracy: 0.9524 - val_loss: 0.2387 - val_accuracy: 0.9362\n",
            "Epoch 142/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1965 - accuracy: 0.9524 - val_loss: 0.2388 - val_accuracy: 0.9362\n",
            "Epoch 143/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1972 - accuracy: 0.9524 - val_loss: 0.2387 - val_accuracy: 0.9362\n",
            "Epoch 144/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1949 - accuracy: 0.9524 - val_loss: 0.2388 - val_accuracy: 0.9362\n",
            "Epoch 145/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1971 - accuracy: 0.9524 - val_loss: 0.2388 - val_accuracy: 0.9362\n",
            "Epoch 146/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1972 - accuracy: 0.9524 - val_loss: 0.2388 - val_accuracy: 0.9362\n",
            "Epoch 147/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1955 - accuracy: 0.9524 - val_loss: 0.2388 - val_accuracy: 0.9362\n",
            "Epoch 148/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1943 - accuracy: 0.9524 - val_loss: 0.2389 - val_accuracy: 0.9362\n",
            "Epoch 149/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1963 - accuracy: 0.9524 - val_loss: 0.2389 - val_accuracy: 0.9362\n",
            "Epoch 150/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1979 - accuracy: 0.9524 - val_loss: 0.2389 - val_accuracy: 0.9362\n",
            "Epoch 151/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1963 - accuracy: 0.9524 - val_loss: 0.2389 - val_accuracy: 0.9362\n",
            "Epoch 152/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1975 - accuracy: 0.9524 - val_loss: 0.2388 - val_accuracy: 0.9362\n",
            "Epoch 153/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.1942 - accuracy: 0.9524 - val_loss: 0.2389 - val_accuracy: 0.9362\n",
            "Epoch 154/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1954 - accuracy: 0.9524 - val_loss: 0.2390 - val_accuracy: 0.9362\n",
            "Epoch 155/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1986 - accuracy: 0.9524 - val_loss: 0.2390 - val_accuracy: 0.9362\n",
            "Epoch 156/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.1955 - accuracy: 0.9524 - val_loss: 0.2390 - val_accuracy: 0.9362\n",
            "Epoch 157/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.1960 - accuracy: 0.9524 - val_loss: 0.2390 - val_accuracy: 0.9362\n",
            "Epoch 158/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1979 - accuracy: 0.9524 - val_loss: 0.2391 - val_accuracy: 0.9362\n",
            "Epoch 159/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1962 - accuracy: 0.9524 - val_loss: 0.2390 - val_accuracy: 0.9362\n",
            "Epoch 160/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1978 - accuracy: 0.9524 - val_loss: 0.2391 - val_accuracy: 0.9362\n",
            "Epoch 161/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1947 - accuracy: 0.9524 - val_loss: 0.2391 - val_accuracy: 0.9362\n",
            "Epoch 162/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.1966 - accuracy: 0.9524 - val_loss: 0.2392 - val_accuracy: 0.9362\n",
            "Epoch 163/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1954 - accuracy: 0.9524 - val_loss: 0.2392 - val_accuracy: 0.9362\n",
            "Epoch 164/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.1963 - accuracy: 0.9524 - val_loss: 0.2392 - val_accuracy: 0.9362\n",
            "Epoch 165/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1959 - accuracy: 0.9524 - val_loss: 0.2392 - val_accuracy: 0.9362\n",
            "Epoch 166/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1955 - accuracy: 0.9524 - val_loss: 0.2393 - val_accuracy: 0.9362\n",
            "Epoch 167/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1964 - accuracy: 0.9524 - val_loss: 0.2393 - val_accuracy: 0.9362\n",
            "Epoch 168/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1964 - accuracy: 0.9524 - val_loss: 0.2394 - val_accuracy: 0.9362\n",
            "Epoch 169/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1937 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 170/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1961 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 171/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1951 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 172/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1945 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 173/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1937 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 174/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1950 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 175/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1929 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 176/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1940 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 177/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1952 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 178/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1922 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 179/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1945 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 180/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1961 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 181/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1940 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 182/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.1928 - accuracy: 0.9524 - val_loss: 0.2398 - val_accuracy: 0.9362\n",
            "Epoch 183/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1939 - accuracy: 0.9524 - val_loss: 0.2398 - val_accuracy: 0.9362\n",
            "Epoch 184/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1960 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 185/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1985 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 186/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1957 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 187/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1967 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 188/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1986 - accuracy: 0.9524 - val_loss: 0.2394 - val_accuracy: 0.9362\n",
            "Epoch 189/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1957 - accuracy: 0.9524 - val_loss: 0.2394 - val_accuracy: 0.9362\n",
            "Epoch 190/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1958 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 191/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1936 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 192/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1954 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 193/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.1944 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 194/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1946 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 195/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.1942 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 196/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.1946 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 197/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1966 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 198/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1942 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 199/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1958 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 200/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1971 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 201/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1943 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 202/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1953 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 203/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1965 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 204/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1946 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 205/500\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.1932 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 206/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1928 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 207/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1956 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 208/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1946 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 209/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1956 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 210/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1948 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 211/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1934 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 212/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1955 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 213/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1971 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 214/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1936 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 215/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1957 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 216/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1949 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 217/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1964 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 218/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1949 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 219/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1959 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 220/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1983 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 221/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1951 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 222/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1937 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 223/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1959 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 224/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1929 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 225/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1942 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 226/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1941 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 227/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1946 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 228/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1932 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 229/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1947 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 230/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1956 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 231/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1955 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 232/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1940 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 233/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1940 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 234/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1966 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 235/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1934 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 236/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1945 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 237/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1927 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 238/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1937 - accuracy: 0.9524 - val_loss: 0.2398 - val_accuracy: 0.9362\n",
            "Epoch 239/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1954 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 240/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1970 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 241/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1947 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 242/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1933 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 243/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1950 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 244/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1960 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 245/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1960 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 246/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1950 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 247/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1963 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 248/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1957 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 249/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1957 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 250/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1958 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 251/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1962 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 252/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1949 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 253/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1939 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 254/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1933 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 255/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1959 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 256/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1946 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 257/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1936 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 258/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1943 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 259/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1936 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 260/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1932 - accuracy: 0.9524 - val_loss: 0.2398 - val_accuracy: 0.9362\n",
            "Epoch 261/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1958 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 262/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1962 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 263/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1976 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 264/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1947 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 265/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1951 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 266/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1940 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 267/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1938 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 268/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1930 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 269/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1953 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 270/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1924 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 271/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1953 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 272/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1942 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 273/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1966 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 274/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1959 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 275/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1938 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 276/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1970 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 277/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1929 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 278/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1937 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 279/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1951 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 280/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1950 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 281/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1961 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 282/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1934 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 283/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1962 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 284/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1951 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 285/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1945 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 286/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1955 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 287/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1939 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 288/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1951 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 289/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1964 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 290/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1958 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 291/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1942 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 292/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1955 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 293/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1950 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 294/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1952 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 295/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1945 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 296/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1965 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 297/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1951 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 298/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1952 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 299/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1950 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 300/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1946 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 301/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1953 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 302/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1954 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 303/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1953 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 304/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1964 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 305/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1944 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 306/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1961 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 307/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1944 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 308/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1944 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 309/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1947 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 310/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1991 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 311/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1932 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 312/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1942 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 313/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1950 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 314/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1972 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 315/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1914 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 316/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1976 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 317/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1968 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 318/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1967 - accuracy: 0.9524 - val_loss: 0.2394 - val_accuracy: 0.9362\n",
            "Epoch 319/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1960 - accuracy: 0.9524 - val_loss: 0.2394 - val_accuracy: 0.9362\n",
            "Epoch 320/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1961 - accuracy: 0.9524 - val_loss: 0.2394 - val_accuracy: 0.9362\n",
            "Epoch 321/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1943 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 322/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1969 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 323/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1941 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 324/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1953 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 325/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1948 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 326/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1949 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 327/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1943 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 328/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1946 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 329/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1952 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 330/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1960 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 331/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1964 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 332/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1948 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 333/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1955 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 334/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1953 - accuracy: 0.9524 - val_loss: 0.2394 - val_accuracy: 0.9362\n",
            "Epoch 335/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1952 - accuracy: 0.9524 - val_loss: 0.2394 - val_accuracy: 0.9362\n",
            "Epoch 336/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1930 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 337/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1937 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 338/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1940 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 339/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1936 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 340/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1950 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 341/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1945 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 342/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1937 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 343/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1921 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 344/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1941 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 345/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1937 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 346/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1947 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 347/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1947 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 348/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1962 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 349/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1950 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 350/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1973 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 351/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1938 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 352/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1951 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 353/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1961 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 354/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1931 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 355/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1946 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 356/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1954 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 357/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1935 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 358/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1949 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 359/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1934 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 360/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1928 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 361/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1933 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 362/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1951 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 363/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1943 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 364/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1950 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 365/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1944 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 366/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1948 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 367/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1938 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 368/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1949 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 369/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1958 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 370/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1942 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 371/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1943 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 372/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1937 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 373/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1956 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 374/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1936 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 375/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1938 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 376/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1936 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 377/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1948 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 378/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1931 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 379/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1962 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 380/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1920 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 381/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1966 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 382/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1941 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 383/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1954 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 384/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1946 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 385/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1962 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 386/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1957 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 387/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1940 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 388/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1963 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 389/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1949 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 390/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1945 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 391/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1934 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 392/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1937 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 393/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1944 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 394/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1943 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 395/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1949 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 396/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1967 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 397/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1934 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 398/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1953 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 399/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1934 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 400/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1944 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 401/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1943 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 402/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1938 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 403/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1953 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 404/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1945 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 405/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1932 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 406/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1940 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 407/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1938 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 408/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1935 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 409/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1931 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 410/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1945 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 411/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1944 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 412/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1962 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 413/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1942 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 414/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1961 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 415/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1956 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 416/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1929 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 417/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1928 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 418/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1940 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 419/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1940 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 420/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1942 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 421/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1944 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 422/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1934 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 423/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1944 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 424/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1958 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 425/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1938 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 426/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1928 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 427/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1937 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 428/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1955 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 429/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1947 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 430/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1949 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 431/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1951 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 432/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1921 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 433/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1934 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 434/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1937 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 435/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1964 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 436/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1942 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 437/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1935 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 438/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1951 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 439/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1949 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 440/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1965 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 441/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1957 - accuracy: 0.9524 - val_loss: 0.2394 - val_accuracy: 0.9362\n",
            "Epoch 442/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1941 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 443/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1934 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 444/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1939 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 445/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1934 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 446/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1949 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 447/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1940 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 448/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1949 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 449/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1929 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 450/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1956 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 451/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1950 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 452/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1930 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 453/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1956 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 454/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1954 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 455/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1950 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 456/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1930 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 457/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1957 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 458/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1955 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 459/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1928 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 460/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1952 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 461/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1946 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 462/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1941 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 463/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1957 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 464/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1949 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 465/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1942 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 466/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1960 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 467/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1950 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 468/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1951 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 469/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1928 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 470/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1922 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 471/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1921 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 472/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1974 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 473/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1957 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 474/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1925 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 475/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1946 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 476/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1935 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 477/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1938 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 478/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1951 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 479/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1942 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 480/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1937 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 481/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1938 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 482/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1942 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 483/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1929 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 484/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1947 - accuracy: 0.9524 - val_loss: 0.2397 - val_accuracy: 0.9362\n",
            "Epoch 485/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1963 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 486/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1955 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 487/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1948 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 488/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1947 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 489/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1939 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 490/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1953 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 491/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1937 - accuracy: 0.9524 - val_loss: 0.2395 - val_accuracy: 0.9362\n",
            "Epoch 492/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1930 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 493/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1957 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 494/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1936 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 495/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1941 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 496/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1944 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 497/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1957 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 498/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1933 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 499/500\n",
            "92/92 [==============================] - 1s 7ms/step - loss: 0.1930 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n",
            "Epoch 500/500\n",
            "92/92 [==============================] - 1s 8ms/step - loss: 0.1934 - accuracy: 0.9524 - val_loss: 0.2396 - val_accuracy: 0.9362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPff-F-HpB0v"
      },
      "source": [
        "#es = EarlyStopping(monitor='val_loss', min_delta=1e-10, patience=400, verbose=1)\n",
        "#rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=15, verbose=1)\n",
        "#mcp = ModelCheckpoint(filepath='weights.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True)\n",
        "\n",
        "#tb = TensorBoard('logs') , callbacks=[es, rlr, mcp, tb]\n",
        "history= model.fit(training_set_scaled, encoded_Y, shuffle=False, epochs=3000, batch_size=32, validation_split=0.3, class_weight=class_weight, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7pYCcVTqVrz",
        "outputId": "1d10f51c-1659-4c34-8526-64ec74755c67"
      },
      "source": [
        "model.evaluate(test_set_scaled, encoded_TY)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "66/66 [==============================] - 0s 3ms/step - loss: 0.3984 - accuracy: 0.8827\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.39839044213294983, 0.8827189803123474]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AxOigTEgGRv",
        "outputId": "e091ce59-3a0e-4cc4-ea68-1b187d6ef7af"
      },
      "source": [
        "yhat = model.predict(test_set_scaled)\n",
        "score = roc_auc_score(encoded_TY, yhat)\n",
        "print('ROC AUC: %.3f' % score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC AUC: 0.506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lv3cG0YoG0rN"
      },
      "source": [
        "prediction= model.predict(test_set_scaled)\n",
        "\n",
        "my_accuracy = accuracy_score(labelY, prediction.round())\n",
        "print(\"My accuracy is=\", my_accuracy)\n",
        "\n",
        "my_precision = precision_score(labelY, prediction.round())\n",
        "print(\"My precesion score is=\", my_precision)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_HLPOh4uaob"
      },
      "source": [
        "# evaluate baseline model with standardized dataset\n",
        "estimators = []\n",
        "estimators.append(('standardize', StandardScaler()))\n",
        "estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=300, batch_size=64, verbose=1)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "results = cross_val_score(pipeline, training_set_scaled, encoded_Y, cv=kfold)\n",
        "print(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0267tELr1z5y",
        "outputId": "8a4f0db6-c956-4bc5-c021-57fc67768671"
      },
      "source": [
        "print(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standardized: 94.61% (0.30%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byJR5EF3hz2U"
      },
      "source": [
        "pred=create_baseline()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUDRE9JB-I6C"
      },
      "source": [
        "#test= testing_set[:, 0:7]\n",
        "print(test)\n",
        "labelY = np.reshape(labelY, (labelY.shape[0],))\n",
        "...\n",
        "# encode class values as integers\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(labelY)\n",
        "encoded_Y = encoder.transform(labelY)\n",
        "#print(labelY[1589:1600])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2x0yOkIK983",
        "outputId": "a5e89290-ff2f-4d18-c8f9-d415c1a95a37"
      },
      "source": [
        "cvscores = []\n",
        "scores = model.evaluate(test_set_scaled, labelY, verbose=1)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "cvscores.append(scores[1] * 100)\n",
        "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "66/66 [==============================] - 0s 3ms/step - loss: 3.1184 - accuracy: 0.8430\n",
            "accuracy: 84.30%\n",
            "84.30% (+/- 0.00%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P_x39B2-I_0"
      },
      "source": [
        "predicted=model.predict(test)\n",
        "print(predicted)\n",
        "plt.plot(predicted)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "vJ3DLeQI-3N-",
        "outputId": "92bfff46-2ebe-487a-b1da-468560af6298"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.savefig('loss.png', dpi=500, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHbt6iB4i9B7"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDMAVa-Wi9vE"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVqRYkeci91E"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctD78S1bptDh"
      },
      "source": [
        "labeltest = testY.astype(int)\n",
        "labeltest = indices_to_one_hot(labeltest, nb_classes)\n",
        "#trainY = trainY[1:]\n",
        "print(testX.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw6SoDir3gyk"
      },
      "source": [
        "#opt = Adam(learning_rate=INIT_LR, decay=(INIT_LR*10 / EPOCHS))\n",
        "#model.compile(loss='binary_crossentropy', optimizer =opt, metrics=['accuracy'])\n",
        "#model.compile(optimizer =RMSprop(learning_rate=0.004), loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QS08dkfQptL1"
      },
      "source": [
        "print(Y_test_pred[1590])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xq_VHMuqistG"
      },
      "source": [
        "import numpy as np\n",
        "nb_classes = 2\n",
        "label = dataset_train[' ATT_FLAG'].astype(int)\n",
        "def indices_to_one_hot(data, nb_classes):\n",
        "    \"\"\"Convert an iterable of indices to one-hot encoded labels.\"\"\"\n",
        "    targets = np.array(data).reshape(-1)\n",
        "    return np.eye(nb_classes)[targets]\n",
        "\n",
        "labeled = indices_to_one_hot(label, nb_classes)\n",
        "labeled = np.reshape(labeled, (labeled.shape[0], labeled.shape[1],))\n",
        "#trainY = trainY[1:]\n",
        "print(labeled.shape)\n",
        "print(training_set_scaled.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f64Ixnc6isw4"
      },
      "source": [
        "seq_size =1\n",
        "print(len(training_set_scaled))\n",
        "def to_sequence(x, seq_size=1):\n",
        "    x_values = []\n",
        "    for i in range (len(x)):\n",
        "        x_values.append(x[i:(i+seq_size)])\n",
        "        \n",
        "    return np.array(x_values)\n",
        "\n",
        "trainX = to_sequence(training_set_scaled, seq_size)\n",
        "testX = to_sequence(test_set_scaled, seq_size)\n",
        "\n",
        "print(trainX.shape)\n",
        "#print(trainY.shape)\n",
        "trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[2],))\n",
        "testX = np.reshape(testX, (testX.shape[0], testX.shape[2],))\n",
        "print(testX.shape)\n",
        "print(trainX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMbp-woNBL5a"
      },
      "source": [
        "\n",
        "# baseline model\n",
        "def create_baseline():\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Conv1D(filters=32, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\", input_shape=(1,7)))\n",
        "\n",
        "  model.add(Dropout(rate=0.2))\n",
        "\n",
        "  model.add(Conv1D(filters=64, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"))\n",
        "  model.add(Dropout(rate=0.2))\n",
        "\n",
        "  model.add(Conv1D(filters=128, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"))\n",
        "  model.add(Dropout(rate=0.2))\n",
        "  model.add(Conv1D(filters=256, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"))\n",
        "  model.add(Dropout(rate=0.2))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(units=128, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(units=1, activation='linear'))\n",
        "\n",
        "\n",
        "  opt = Adam(learning_rate=INIT_LR, decay=(INIT_LR*10 / EPOCHS))\n",
        "  model.compile(optimizer= opt, loss=\"mae\", metrics=['accuracy'])\n",
        "  return model\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}